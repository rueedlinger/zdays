{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate data with Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum entropy classifier with a semi-supervised learning approach to detect namend entities in a German text\n",
    "\n",
    "This example uses a max entropy classifier which was bootstrapped with a wordlist for labeling a unlabeled corpus. A good overview about different learning technics and specially about the semi-supervised approach can be found in the paper form Nadeau et Al.\n",
    "\n",
    "```\n",
    "Nadeau, David, and Satoshi Sekine. \"A survey of named entity recognition and classification.\" Lingvisticae Investigationes 30.1 (2007): 3-26.\n",
    "```\n",
    "\n",
    "This is my naive approach to Namend Entity Recognition. The goal is to extract 'technical' entities like Java, .Net, etc. from a text.\n",
    "\n",
    "In this context semi-supervised learning means:\n",
    "- start with a bootstrap algorithm which labels the unlabeled corpus with a two wordlists. A label TECH for technical entities and NONTECH for words which are not technical entities.\n",
    "- new labeled corpus is used to train the classifier \n",
    "\n",
    "<h3>Notice</h3>\n",
    "\n",
    "This is just a playground example, for better results you may have to use a better corpus and modify the feature extraction function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Corpus</h3>\n",
    "\n",
    "I used a simple corpus with some sentences from different Wikipedia articles about different programming languages. So keep in mind that the F-Score is not really meaningful. To get a better result use a larger and domain specific corpus for which you want to train entities.\n",
    "http://data.stackexchange.com/stackoverflow/query/edit/368452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Body\n",
      "0  <p>After I have migrated my project from VS201...\n",
      "1  <p>How does GHC handle thunks that are accesse...\n",
      "2  <p>An example usage:</p>\\n\\n<p><a href=\"http:/...\n",
      "3  <p>With the new PreferenceFragmentCompat from ...\n",
      "4  <p>I've always programed android with Eclipse ...\n"
     ]
    }
   ],
   "source": [
    "posts = pd.read_csv('data/stackoverflow_posts.csv')\n",
    "\n",
    "print posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sentences', 24185)\n"
     ]
    }
   ],
   "source": [
    "content = ''\n",
    "for i in posts.index:\n",
    "    content += posts.ix[i]['Body']\n",
    "\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "blob = TextBlob(soup.get_text()) \n",
    "\n",
    "print('sentences', len(blob.sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>feature extraction</h3>\n",
    "\n",
    "So the following table describes the features which will be extracted and used for named entity recognition.\n",
    "\n",
    "feature|description\n",
    "---|---|---\n",
    "word|current word (eg. Java)\n",
    "word-1|the previous word \n",
    "word+1|the next word\n",
    "length|length of the current word\n",
    "isupper|true if current word has at least one cased character\n",
    "special-char|if current word contains special char (eg. +, -, etc.)\n",
    "\n",
    "\n",
    "A addional feautre could be POS (part of spech tag).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def contains_word(word, wordlist):\n",
    "    for w in wordlist:\n",
    "        if w.lower() in word.lower():\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def contains_digits(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "def extract_features(word, i, sentence):\n",
    "    \n",
    "    features = {\"word\": word,\n",
    "                \"length\": len(word),\n",
    "                \"isupper\": word.isupper(),\n",
    "                \"special-char\": contains_word(word, ['+', '-', '.', '#']),\n",
    "                \"contains-digit\": contains_digits(word)}\n",
    "    \n",
    "    # token - 1\n",
    "    if i == 0:\n",
    "        features[\"word-1\"] = \"<START>\"    \n",
    "    else:\n",
    "        features[\"word-1\"] = sentence[i-1]\n",
    "        \n",
    "    \n",
    "    # token + 1\n",
    "    if (i+1) < len(sentence):\n",
    "         features[\"word+1\"] = sentence[i+1]\n",
    "         \n",
    "    else:\n",
    "        features[\"word+1\"] = \"<END>\"\n",
    "        \n",
    "    \n",
    "    return features \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bootstrapping</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features_from_text(blob, wordlist, max=1000):\n",
    "\n",
    "    featuresets = []\n",
    "    \n",
    "    \n",
    "    s_count = 0\n",
    "    w_count = 0\n",
    "    max_count = 0\n",
    "    \n",
    "    for s in blob.sentences:\n",
    "    \n",
    "        for w in s.words:\n",
    "            if max_count >= max:\n",
    "                break\n",
    "        \n",
    "            if w.lower() in map(lambda x:x.lower(), wordlist):\n",
    "                featuresets.append(extract_features(w, w_count, s.words))\n",
    "                max_count += 1\n",
    "            \n",
    "            #featuresets.append((extract_features(blob.sentences[s_count], w_count), get_label(w, wordlist, \"TECH\")))\n",
    "            w_count += 1\n",
    "            \n",
    "            \n",
    "        s_count +=1\n",
    "        w_count = 0\n",
    "\n",
    "    \n",
    "    return featuresets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word\n",
      "0  the\n",
      "1   of\n",
      "2   to\n",
      "3  and\n",
      "4    a\n",
      "('NONTECH', 1000)\n"
     ]
    }
   ],
   "source": [
    "words = pd.read_csv('data/top1000en.txt', header=None);\n",
    "words.columns = ['Word']\n",
    "print words.head()\n",
    "\n",
    "nonefeats = get_features_from_text(blob, words['Word'].values)\n",
    "\n",
    "print('NONTECH', len(nonefeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id     TagName   Count  ExcerptPostId  WikiPostId\n",
      "0   1        .net  216781        3624959     3607476\n",
      "1   2        html  453768        3673183     3673182\n",
      "2   3  javascript  937830        3624960     3607052\n",
      "3   4         css  332802        3644670     3644669\n",
      "4   5         php  803687        3624936     3607050\n",
      "('TECH', 1000)\n"
     ]
    }
   ],
   "source": [
    "tech = pd.read_csv('data/top_stackoverflow_tags.csv');\n",
    "print tech.head()\n",
    "\n",
    "\n",
    "techfeats = get_features_from_text(blob, tech['TagName'].values)\n",
    "\n",
    "print('TECH', len(techfeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'word': u'LINQ', 'contains-digit': False, 'special-char': False, 'word-1': u'following', 'length': 4, 'word+1': u'statement', 'isupper': True}, 'TECH')\n"
     ]
    }
   ],
   "source": [
    "featureset_tech = [(f, 'TECH') for f in techfeats]\n",
    "print featureset_tech[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'word': u'After', 'contains-digit': False, 'special-char': False, 'word-1': '<START>', 'length': 5, 'word+1': u'I', 'isupper': False}, 'NONTECH')\n"
     ]
    }
   ],
   "source": [
    "featureset_nontech = [(f, 'NONTECH') for f in nonefeats]\n",
    "print featureset_nontech[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "train with 750 TECH instances, train with 750 NONE instances\n"
     ]
    }
   ],
   "source": [
    "techcutoff = len(featureset_tech)*3/4\n",
    "nonecutoff = len(featureset_nontech)*3/4\n",
    "\n",
    "\n",
    "\n",
    "trainfeats = featureset_tech[:techcutoff] + featureset_nontech[:nonecutoff]\n",
    "testfeats = featureset_tech[techcutoff:] + featureset_nontech[nonecutoff:]\n",
    "\n",
    "\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "print 'train with %d TECH instances, train with %d NONE instances' % (techcutoff, nonecutoff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 0.902)\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.MaxentClassifier.train(trainfeats, algorithm=\"GIS\", trace=0)\n",
    "print('accuracy', nltk.classify.accuracy(classifier, testfeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>\n",
    "\n",
    "The following method evaluates the classifier and returns:\n",
    "\n",
    "- F-measure\n",
    "- precision \n",
    "- recall \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_clf(classifier, testfeats):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    \n",
    "    labels = {}\n",
    "    \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "        \n",
    "        \n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        \n",
    "        testsets[observed].add(i)\n",
    "        labels[label] = True\n",
    "        \n",
    "    print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    \n",
    "    for label in labels.keys():\n",
    "        print\n",
    "        print 'label:', label\n",
    "        print 'precision:', nltk.metrics.precision(refsets[label], testsets[label])\n",
    "        print 'recall:', nltk.metrics.recall(refsets[label], testsets[label])\n",
    "        print 'F-measure:', nltk.metrics.f_measure(refsets[label], testsets[label])\n",
    "        \n",
    "        #print 'size', len(refsets[label])\n",
    "        \n",
    "        \n",
    "    \n",
    "    print \n",
    "    print \"most_informative_features:\"\n",
    "    \n",
    "    classifier.show_most_informative_features()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.902\n",
      "\n",
      "label: TECH\n",
      "precision: 0.857651245552\n",
      "recall: 0.964\n",
      "F-measure: 0.907721280603\n",
      "\n",
      "label: NONTECH\n",
      "precision: 0.958904109589\n",
      "recall: 0.84\n",
      "F-measure: 0.89552238806\n",
      "\n",
      "most_informative_features:\n",
      "  -7.152 word==u'go' and label is 'NONTECH'\n",
      "  -4.687 word==u'function' and label is 'NONTECH'\n",
      "   4.171 word==u'those' and label is 'NONTECH'\n",
      "   4.051 word==u'reason' and label is 'NONTECH'\n",
      "   3.711 word==u'decided' and label is 'NONTECH'\n",
      "  -3.665 word+1==u'Point' and label is 'TECH'\n",
      "  -3.644 length==2 and label is 'TECH'\n",
      "  -3.524 word==u'file' and label is 'NONTECH'\n",
      "   3.292 word+1==u'removed' and label is 'NONTECH'\n",
      "   3.254 word==u'call' and label is 'NONTECH'\n"
     ]
    }
   ],
   "source": [
    "eval_clf(classifier, testfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label new data</h3>\n",
    "\n",
    "The following example tries to label a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where (NONTECH)\n",
      "can (NONTECH)\n",
      "i (NONTECH)\n",
      "find (NONTECH)\n",
      "a (NONTECH)\n",
      "good (NONTECH)\n",
      "PHP (TECH)\n",
      "or (NONTECH)\n",
      "Ruby (TECH)\n",
      "Tutorial (TECH)\n",
      "\n",
      "Should (NONTECH)\n",
      "I (NONTECH)\n",
      "use (NONTECH)\n",
      "Scrum (TECH)\n",
      "or (NONTECH)\n",
      "Kanban (TECH)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Where can i find a good PHP or Ruby Tutorial? Should I use Scrum or Kanban.\"\n",
    "blob = TextBlob(new_text)\n",
    "\n",
    "\n",
    "for s in blob.sentences:\n",
    "    w_count = 0\n",
    "    for w in s.words:\n",
    "        print w + ' (' + classifier.classify(extract_features(w, w_count, s.words)) + ')'\n",
    "        w_count += 1\n",
    "    print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
